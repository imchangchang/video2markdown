"""Stage 5: AI å›¾åƒåˆ†æ.

è¾“å…¥: è§†é¢‘æ–‡ä»¶ + KeyFrames (M2) + VideoTranscript (M1)
è¾“å‡º: ImageDescriptions (M3)

æµç¨‹:
    1. æ ¹æ® M2 çš„æ—¶é—´ç‚¹æå–åŸå§‹å¸§ (æ— å‹ç¼©)
    2. ä½¿ç”¨ Kimi Vision API åˆ†ææ¯å¼ å›¾ç‰‡
    3. ç»“åˆ M1 çš„æ–‡å­—ç¨¿ä¸Šä¸‹æ–‡
    4. ç”Ÿæˆå›¾ç‰‡æè¿°
"""

import base64
from pathlib import Path
from typing import Optional

import cv2
from openai import OpenAI

from video2markdown.config import settings
from video2markdown.models import ImageDescription, ImageDescriptions, KeyFrames, VideoTranscript


def analyze_images(
    video_path: Path,
    keyframes: KeyFrames,
    transcript: VideoTranscript,
    output_dir: Path,
    max_size: int = 1024,
) -> ImageDescriptions:
    """AI åˆ†æå…³é”®å¸§å›¾ç‰‡.
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        keyframes: ç­›é€‰åçš„å…³é”®å¸§ (M2)
        transcript: è§†é¢‘æ–‡å­—ç¨¿ (M1)
        output_dir: è¾“å‡ºç›®å½• (ä¿å­˜åŸå§‹å¸§)
        max_size: å‘é€ç»™ API çš„æœ€å¤§å›¾ç‰‡å°ºå¯¸
        
    Returns:
        ImageDescriptions (M3)
    """
    print(f"[Stage 5] AI å›¾åƒåˆ†æ: {len(keyframes.frames)} å¼ å›¾ç‰‡")
    
    client = OpenAI(**settings.get_client_kwargs())
    descriptions = []
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    for i, frame in enumerate(keyframes.frames, 1):
        print(f"  åˆ†æå›¾ç‰‡ {i}/{len(keyframes.frames)} @ {frame.timestamp:.1f}s...")
        
        # 1. æå–åŸå§‹å¸§ (é«˜è´¨é‡)
        frame_path = output_dir / f"frame_{i:04d}_{frame.timestamp:.1f}s.jpg"
        _extract_original_frame(video_path, frame.timestamp, frame_path)
        
        # 2. å‡†å¤‡ API è°ƒç”¨ (å‹ç¼©ç‰ˆæœ¬)
        api_image = _prepare_for_api(frame_path, max_size)
        
        # 3. è·å–ç›¸å…³æ–‡å­—ç¨¿
        context = transcript.get_text_around(frame.timestamp, window=10.0)
        
        # 4. è°ƒç”¨ AI åˆ†æ
        desc = _analyze_single_image(
            client, api_image, frame.timestamp, frame_path, context
        )
        
        descriptions.append(desc)
        print(f"    âœ“ {desc.description[:60]}...")
    
    print(f"  âœ“ å®Œæˆ {len(descriptions)} å¼ å›¾ç‰‡åˆ†æ")
    return ImageDescriptions(descriptions=descriptions)


def _extract_original_frame(
    video_path: Path,
    timestamp: float,
    output_path: Path,
    quality: int = 95,
) -> Path:
    """æå–åŸå§‹è§†é¢‘å¸§ (æ— å‹ç¼©ï¼Œé«˜è´¨é‡)."""
    cap = cv2.VideoCapture(str(video_path))
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_idx = int(timestamp * fps)
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
    
    ret, frame = cap.read()
    cap.release()
    
    if not ret:
        raise RuntimeError(f"æ— æ³•è¯»å– {timestamp}s çš„å¸§")
    
    # ä¿å­˜é«˜è´¨é‡åŸå›¾
    cv2.imwrite(str(output_path), frame, [cv2.IMWRITE_JPEG_QUALITY, quality])
    return output_path


def _prepare_for_api(image_path: Path, max_size: int) -> Path:
    """å‡†å¤‡å›¾ç‰‡ç”¨äº API è°ƒç”¨ (å‹ç¼©ä½†ä¿æŒæ¸…æ™°)."""
    img = cv2.imread(str(image_path))
    if img is None:
        raise RuntimeError(f"æ— æ³•è¯»å–å›¾ç‰‡: {image_path}")
    
    h, w = img.shape[:2]
    
    # ç­‰æ¯”ä¾‹ç¼©æ”¾
    if max(h, w) > max_size:
        scale = max_size / max(h, w)
        new_w = int(w * scale)
        new_h = int(h * scale)
        img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)
    
    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
    temp_path = image_path.parent / f"{image_path.stem}_api.jpg"
    cv2.imwrite(str(temp_path), img, [cv2.IMWRITE_JPEG_QUALITY, 85])
    
    return temp_path


def _load_prompt_with_meta(template_path: Path):
    """åŠ è½½ prompt æ¨¡æ¿ï¼Œè¿”å› (system_msg, user_template, api_params)."""
    import yaml
    
    content = template_path.read_text(encoding="utf-8")
    
    # è§£æ YAML frontmatter
    _, frontmatter, body = content.split("---", 2)
    metadata = yaml.safe_load(frontmatter)
    
    system_msg = metadata.get("system", "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è§†é¢‘å†…å®¹åˆ†æå¸ˆã€‚")
    api_params = metadata.get("parameters", {})
    user_template = body.strip()
    
    return system_msg, user_template, api_params


def _print_usage_info(response) -> None:
    """æ‰“å° API ç”¨é‡å’Œä»·æ ¼ä¿¡æ¯ï¼Œå¹¶æ›´æ–°å…¨å±€ç»Ÿè®¡."""
    if not hasattr(response, 'usage') or response.usage is None:
        return
    
    usage = response.usage
    prompt_tokens = getattr(usage, 'prompt_tokens', 0)
    completion_tokens = getattr(usage, 'completion_tokens', 0)
    total_tokens = getattr(usage, 'total_tokens', 0)
    
    if total_tokens == 0:
        return
    
    # æ›´æ–°å…¨å±€ç»Ÿè®¡
    from video2markdown.stats import get_stats
    get_stats().add(prompt_tokens, completion_tokens)
    
    # ä»é…ç½®è·å–ä»·æ ¼
    input_cost = (prompt_tokens / 1_000_000) * settings.price_input_per_1m
    output_cost = (completion_tokens / 1_000_000) * settings.price_output_per_1m
    total_cost = input_cost + output_cost
    
    print(f"      ğŸ“Š Token ç”¨é‡: {prompt_tokens:,} è¾“å…¥ / {completion_tokens:,} è¾“å‡º")
    print(f"      ğŸ’° é¢„ä¼°è´¹ç”¨: Â¥{total_cost:.4f}")


def _analyze_single_image(
    client: OpenAI,
    image_path: Path,
    timestamp: float,
    original_path: Path,
    context: str,
) -> ImageDescription:
    """ä½¿ç”¨ Kimi Vision API åˆ†æå•å¼ å›¾ç‰‡."""
    
    # è¯»å–å¹¶ç¼–ç å›¾ç‰‡
    with open(image_path, "rb") as f:
        image_data = base64.b64encode(f.read()).decode("utf-8")
    
    # åŠ è½½ prompt æ¨¡æ¿
    prompt_path = settings.prompts_dir / "image_analysis.md"
    if not prompt_path.exists():
        raise FileNotFoundError(f"Prompt æ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}")
    
    system_msg, user_template, api_params = _load_prompt_with_meta(prompt_path)
    user_content = user_template.format(context=context[:500])
    
    # è°ƒç”¨ API
    response = client.chat.completions.create(
        model=settings.vision_model,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": [
                {"type": "text", "text": user_content},
                {"type": "image_url", "image_url": {
                    "url": f"data:image/jpeg;base64,{image_data}"
                }}
            ]}
        ],
        **api_params,
    )
    
    content = response.choices[0].message.content
    
    # æ‰“å° Token ç”¨é‡
    _print_usage_info(response)
    
    # è§£æå“åº” (ç®€å•å¤„ç†)
    description = content.strip()
    key_elements = _extract_key_elements(content)
    
    # æ¸…ç†ä¸´æ—¶ API å›¾ç‰‡
    image_path.unlink(missing_ok=True)
    
    return ImageDescription(
        timestamp=timestamp,
        image_path=original_path,  # æŒ‡å‘åŸå§‹é«˜è´¨é‡å›¾ç‰‡
        description=description,
        key_elements=key_elements,
        related_transcript=context,
    )


def _extract_key_elements(text: str) -> list[str]:
    """ä»æè¿°ä¸­æå–å…³é”®å…ƒç´ ."""
    # ç®€å•æå–ï¼šå¯»æ‰¾å…³é”®è¯æˆ–åˆ—è¡¨é¡¹
    elements = []
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åˆ—è¡¨æ ¼å¼
    lines = text.split("\n")
    for line in lines:
        line = line.strip()
        # æå–ä»¥ - æˆ–æ•°å­—å¼€å¤´çš„åˆ—è¡¨é¡¹
        if line.startswith("-") or line.startswith("â€¢"):
            elements.append(line[1:].strip())
    
    return elements[:5]  # æœ€å¤š 5 ä¸ª


# CLI å…¥å£
if __name__ == "__main__":
    import sys
    from video2markdown.stage1_analyze import analyze_video
    from video2markdown.stage2_transcribe import transcribe_video
    from video2markdown.stage3_keyframes import extract_candidate_frames
    from video2markdown.stage4_filter import filter_keyframes
    
    if len(sys.argv) < 3:
        print("ç”¨æ³•: python -m video2markdown.stage5_analyze_images <è§†é¢‘æ–‡ä»¶> <æ¨¡å‹è·¯å¾„>")
        sys.exit(1)
    
    video_path = Path(sys.argv[1])
    model_path = Path(sys.argv[2])
    
    # è¿è¡Œå‰ç½®é˜¶æ®µ
    video_info = analyze_video(video_path)
    transcript = transcribe_video(video_path, video_info, model_path)
    candidates = extract_candidate_frames(video_path, video_info)
    keyframes = filter_keyframes(video_path, candidates, transcript)
    
    # è¿è¡Œ Stage 5
    output_dir = Path("testbench/output") / f"{video_path.stem}_frames"
    descriptions = analyze_images(video_path, keyframes, transcript, output_dir)
    
    print(f"\nå›¾ç‰‡åˆ†æç»“æœ (M3):")
    for desc in descriptions.descriptions:
        print(f"\n  [{desc.timestamp:.1f}s] {desc.image_path.name}")
        print(f"  æè¿°: {desc.description[:100]}...")
        print(f"  å…ƒç´ : {desc.key_elements}")
