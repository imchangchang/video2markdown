"""Stage 2: éŸ³é¢‘æå–ä¸æ–‡ç¨¿ç”Ÿæˆ.

åŒ…å«ä¸‰ä¸ªå­é˜¶æ®µ:
    2a: éŸ³é¢‘æå–
    2b: è¯­éŸ³è½¬å½• (åŸå§‹å£è¯­åŒ–æ–‡æœ¬)
    2c: AIæ–‡ç¨¿ä¼˜åŒ– (ç”Ÿæˆå¯ç›´æ¥é˜…è¯»çš„ M1)

è¾“å…¥: è§†é¢‘æ–‡ä»¶è·¯å¾„ + VideoInfo
è¾“å‡º: VideoTranscript (M1) + SRT (åŸå§‹è½¬å½•ï¼Œå‚è€ƒç”¨)
"""

import json
import subprocess
import tempfile
from pathlib import Path
from typing import Optional

import opencc
from openai import OpenAI

from video2markdown.config import settings
from video2markdown.models import TranscriptSegment, VideoInfo, VideoTranscript


def extract_audio(video_path: Path, output_path: Path) -> Path:
    """Stage 2a: ä»è§†é¢‘æå–éŸ³é¢‘."""
    print(f"  [2a] æå–éŸ³é¢‘...")
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    cmd = [
        "ffmpeg", "-y", "-i", str(video_path),
        "-vn",
        "-acodec", "pcm_s16le",
        "-ar", "16000",
        "-ac", "1",
        str(output_path)
    ]
    
    subprocess.run(cmd, check=True, capture_output=True)
    print(f"  âœ“ éŸ³é¢‘å·²æå–: {output_path}")
    return output_path


def transcribe_audio(
    audio_path: Path,
    model_path: Path,
    language: str = "zh"
) -> list[TranscriptSegment]:
    """Stage 2b: ä½¿ç”¨ whisper.cpp è½¬å½•éŸ³é¢‘."""
    print(f"  [2b] è¯­éŸ³è½¬å½• (ä½¿ç”¨ {model_path.name})...")
    
    whisper_cli = _find_whisper_cli()
    output_dir = audio_path.parent
    output_name = audio_path.stem
    output_json = output_dir / f"{output_name}.json"
    
    cmd = [
        str(whisper_cli),
        "-m", str(model_path),
        "-f", str(audio_path),
        "-oj",
        "-of", str(output_dir / output_name),
        "-l", language,
    ]
    
    print(f"    è¿è¡Œ: whisper-cli -m {model_path.name} ...")
    result = subprocess.run(cmd, capture_output=True, text=True)
    
    if result.returncode != 0:
        print(f"  é”™è¯¯: {result.stderr}")
        raise RuntimeError(f"whisper-cli å¤±è´¥: {result.returncode}")
    
    # æŸ¥æ‰¾è¾“å‡ºæ–‡ä»¶
    if not output_json.exists():
        candidates = [
            audio_path.with_suffix(".json"),
            Path(str(audio_path) + ".json"),
            output_dir / f"{output_name}.wav.json",
        ]
        for candidate in candidates:
            if candidate.exists():
                output_json = candidate
                break
        else:
            raise FileNotFoundError(f"è½¬å½•è¾“å‡ºä¸å­˜åœ¨: {candidates}")
    
    # è§£æ
    with open(output_json, "r") as f:
        data = json.load(f)
    
    output_json.unlink(missing_ok=True)
    
    segments = []
    for seg in data.get("transcription", []):
        segments.append(TranscriptSegment(
            start=seg.get("offsets", {}).get("from", 0) / 1000.0,
            end=seg.get("offsets", {}).get("to", 0) / 1000.0,
            text=seg.get("text", "").strip(),
        ))
    
    print(f"  âœ“ è½¬å½•å®Œæˆ: {len(segments)} ä¸ªç‰‡æ®µ")
    return segments


def load_prompt(template_path: Path, **kwargs) -> str:
    """åŠ è½½ prompt æ¨¡æ¿å¹¶å¡«å……å˜é‡."""
    import yaml
    
    content = template_path.read_text(encoding="utf-8")
    
    # è§£æ YAML frontmatter
    if content.startswith("---"):
        _, frontmatter, body = content.split("---", 2)
        metadata = yaml.safe_load(frontmatter)
        # æå– body éƒ¨åˆ†ï¼ˆå»æ‰ frontmatterï¼‰
        content = body.strip()
    
    # å¡«å……å˜é‡
    return content.format(**kwargs)


def optimize_transcript(
    segments: list[TranscriptSegment],
    title: str,
    language: str = "zh",
) -> str:
    """Stage 2c: AI ä¼˜åŒ–è½¬å½•ä¸ºå¯è¯»æ–‡ç¨¿ (ç”Ÿæˆ M1).
    
    å°†å£è¯­åŒ–çš„è½¬å½•æ–‡æœ¬è½¬æ¢ä¸ºç»“æ„åŒ–çš„å¯è¯»æ–‡ç¨¿.
    Prompt ä» prompts/transcript_optimization.md åŠ è½½.
    """
    print(f"  [2c] AI æ–‡ç¨¿ä¼˜åŒ–...")
    
    # åˆå¹¶è½¬å½•æ–‡æœ¬
    raw_text = "\n".join(f"[{int(seg.start//60):02d}:{int(seg.start%60):02d}] {seg.text}" 
                         for seg in segments)
    
    # åŠ è½½ prompt æ¨¡æ¿
    prompt_path = settings.prompts_dir / "transcript_optimization.md"
    if not prompt_path.exists():
        raise FileNotFoundError(f"Prompt æ–‡ä»¶ä¸å­˜åœ¨: {prompt_path}")
    
    prompt = load_prompt(
        prompt_path,
        title=title,
        raw_text=raw_text[:8000]  # é™åˆ¶é•¿åº¦
    )
    
    # ä» prompt frontmatter è·å–å‚æ•°
    import yaml
    prompt_meta = yaml.safe_load(
        prompt_path.read_text(encoding="utf-8").split("---")[1]
    )
    api_params = prompt_meta.get("parameters", {})
    system_msg = prompt_meta.get("system", "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ–‡ç¨¿ç¼–è¾‘ã€‚")
    
    client = OpenAI(**settings.get_client_kwargs())
    
    response = client.chat.completions.create(
        model=settings.model,
        messages=[
            {"role": "system", "content": system_msg},
            {"role": "user", "content": prompt}
        ],
        **api_params,
    )
    
    optimized = response.choices[0].message.content.strip()
    
    # æ¸…ç†å¯èƒ½çš„ä»£ç å—æ ‡è®°
    if optimized.startswith("```markdown"):
        optimized = optimized[11:].strip()
    if optimized.startswith("```"):
        optimized = optimized[3:].strip()
    if optimized.endswith("```"):
        optimized = optimized[:-3].strip()
    
    print(f"  âœ“ æ–‡ç¨¿ä¼˜åŒ–å®Œæˆ")
    return optimized


def convert_to_simplified(text: str) -> str:
    """ç¹ä½“ä¸­æ–‡è½¬ç®€ä½“ä¸­æ–‡."""
    converter = opencc.OpenCC('t2s')
    return converter.convert(text)


def transcribe_video(
    video_path: Path,
    video_info: VideoInfo,
    model_path: Path,
    language: str = "zh",
    temp_dir: Optional[Path] = None,
    cache_dir: Optional[Path] = None,
    use_cache: bool = True,
) -> VideoTranscript:
    """Stage 2 ä¸»å‡½æ•°: å®Œæ•´çš„éŸ³é¢‘æå–ã€è½¬å½•ã€ä¼˜åŒ–æµç¨‹.
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        video_info: è§†é¢‘ä¿¡æ¯
        model_path: Whisper æ¨¡å‹è·¯å¾„
        language: è¯­è¨€ä»£ç 
        temp_dir: ä¸´æ—¶ç›®å½•
        cache_dir: ç¼“å­˜ç›®å½•ï¼ˆç”¨äºä¿å­˜è½¬å½•ç»“æœï¼Œé¿å…é‡å¤æ‰§è¡Œï¼‰
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        
    Returns:
        VideoTranscript (M1) - AIä¼˜åŒ–åçš„å¯è¯»æ–‡ç¨¿
    """
    print(f"[Stage 2] éŸ³é¢‘æå–ä¸æ–‡ç¨¿ç”Ÿæˆ: {video_path.name}")
    
    # è®¾ç½®ç¼“å­˜ç›®å½•
    if cache_dir is None:
        cache_dir = settings.temp_dir / "cache" / "stage2"
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    # ç”Ÿæˆç¼“å­˜é”®ï¼ˆåŸºäºè§†é¢‘æ–‡ä»¶å“ˆå¸Œï¼‰
    import hashlib
    video_hash = hashlib.sha256(video_path.read_bytes()[:1024*1024]).hexdigest()[:16]
    cache_key = f"{video_path.stem}_{video_hash}_{model_path.name}_{language}"
    cache_path = cache_dir / f"{cache_key}_raw.json"
    
    # æ£€æŸ¥ç¼“å­˜
    if use_cache and cache_path.exists():
        print(f"  ğŸ“¦ å‘ç°ç¼“å­˜ï¼ŒåŠ è½½ä¹‹å‰çš„è½¬å½•ç»“æœ...")
        import json
        with open(cache_path, "r", encoding="utf-8") as f:
            cached = json.load(f)
        
        segments = [TranscriptSegment(**seg) for seg in cached["segments"]]
        print(f"  âœ“ ä»ç¼“å­˜åŠ è½½: {len(segments)} ä¸ªç‰‡æ®µ")
        
        # 2c: AI æ–‡ç¨¿ä¼˜åŒ– (ç”Ÿæˆ M1) - è¿™éƒ¨åˆ†ä¸ç¼“å­˜ï¼Œæ¯æ¬¡éƒ½é‡æ–°ä¼˜åŒ–
        optimized_text = optimize_transcript(segments, video_path.stem, language)
        
        transcript = VideoTranscript(
            video_path=video_path,
            title=video_path.stem,
            language=language,
            segments=segments,
            optimized_text=optimized_text,
        )
        
        print(f"  âœ“ M1 (è§†é¢‘æ–‡ç¨¿) ç”Ÿæˆå®Œæˆ")
        print(f"    - åŸå§‹è½¬å½•: {len(segments)} ä¸ªç‰‡æ®µ (æ¥è‡ªç¼“å­˜)")
        print(f"    - ä¼˜åŒ–æ–‡ç¨¿: {len(optimized_text)} å­—ç¬¦")
        
        return transcript
    
    # åˆ›å»ºä¸´æ—¶ç›®å½•
    if temp_dir is None:
        temp_dir = Path(tempfile.gettempdir()) / "video2markdown"
    temp_dir.mkdir(parents=True, exist_ok=True)
    
    audio_path = temp_dir / f"{video_path.stem}.wav"
    
    try:
        # 2a: æå–éŸ³é¢‘
        extract_audio(video_path, audio_path)
        
        # 2b: è¯­éŸ³è½¬å½•
        segments = transcribe_audio(audio_path, model_path, language)
        
        # ç¹ç®€è½¬æ¢
        if language in ("zh", "auto"):
            for seg in segments:
                seg.text = convert_to_simplified(seg.text)
        
        # ä¿å­˜ç¼“å­˜ï¼ˆåŸå§‹è½¬å½•ç»“æœï¼‰
        if use_cache:
            import json
            cache_data = {
                "video_path": str(video_path),
                "video_hash": video_hash,
                "model": str(model_path),
                "language": language,
                "segments": [seg.to_dict() for seg in segments]
            }
            with open(cache_path, "w", encoding="utf-8") as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            print(f"  ğŸ’¾ è½¬å½•ç»“æœå·²ç¼“å­˜: {cache_path}")
        
        # 2c: AI æ–‡ç¨¿ä¼˜åŒ– (ç”Ÿæˆ M1)
        optimized_text = optimize_transcript(segments, video_path.stem, language)
        
        # åˆ›å»º VideoTranscript (M1)
        transcript = VideoTranscript(
            video_path=video_path,
            title=video_path.stem,
            language=language,
            segments=segments,
            optimized_text=optimized_text,
        )
        
        print(f"  âœ“ M1 (è§†é¢‘æ–‡ç¨¿) ç”Ÿæˆå®Œæˆ")
        print(f"    - åŸå§‹è½¬å½•: {len(segments)} ä¸ªç‰‡æ®µ")
        print(f"    - ä¼˜åŒ–æ–‡ç¨¿: {len(optimized_text)} å­—ç¬¦")
        
        return transcript
        
    finally:
        audio_path.unlink(missing_ok=True)


def _find_whisper_cli() -> Path:
    """æŸ¥æ‰¾ whisper-cli å¯æ‰§è¡Œæ–‡ä»¶."""
    cli_path = settings.resolve_whisper_cli()
    if cli_path:
        return cli_path
    
    candidates = [
        Path(__file__).parent.parent.parent / "whisper.cpp" / "build" / "bin" / "whisper-cli",
        Path("whisper-cli"),
        Path("/usr/local/bin/whisper-cli"),
    ]
    
    for path in candidates:
        if path.exists() and path.is_file():
            return path.absolute()
    
    for cmd in ["whisper-cli", "whisper-cpp"]:
        result = subprocess.run(["which", cmd], capture_output=True, text=True)
        if result.returncode == 0:
            return Path(result.stdout.strip())
    
    raise FileNotFoundError("whisper-cli not found. Build: cd whisper.cpp && cmake -B build && cmake --build build")


# CLI å…¥å£
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Stage 2: éŸ³é¢‘æå–ä¸æ–‡ç¨¿ç”Ÿæˆ")
    parser.add_argument("video_path", type=Path, help="è§†é¢‘æ–‡ä»¶è·¯å¾„")
    parser.add_argument("model_path", type=Path, help="Whisper æ¨¡å‹è·¯å¾„")
    parser.add_argument("--no-cache", action="store_true", help="ä¸ä½¿ç”¨ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æ–°è½¬å½•")
    parser.add_argument("--clear-cache", action="store_true", help="æ¸…é™¤ç¼“å­˜åæ‰§è¡Œ")
    args = parser.parse_args()
    
    # æ¸…é™¤ç¼“å­˜ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
    if args.clear_cache:
        cache_dir = settings.temp_dir / "cache" / "stage2"
        if cache_dir.exists():
            import shutil
            shutil.rmtree(cache_dir)
            print(f"ğŸ—‘ï¸  å·²æ¸…é™¤ç¼“å­˜: {cache_dir}")
    
    from video2markdown.stage1_analyze import analyze_video
    video_info = analyze_video(args.video_path)
    
    transcript = transcribe_video(
        args.video_path, 
        video_info, 
        args.model_path,
        use_cache=not args.no_cache
    )
    
    # ä¿å­˜è¾“å‡º
    output_dir = settings.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜ SRT (åŸå§‹è½¬å½•ï¼Œå‚è€ƒç”¨)
    srt_path = output_dir / f"{args.video_path.stem}.srt"
    srt_path.write_text(transcript.to_srt(), encoding="utf-8")
    print(f"\n  SRT (åŸå§‹è½¬å½•): {srt_path}")
    
    # ä¿å­˜ M1 (AIä¼˜åŒ–åçš„æ–‡ç¨¿ï¼Œæ ¸å¿ƒäº§ç‰©)
    m1_path = output_dir / f"{args.video_path.stem}_word.md"
    m1_content = f"# {transcript.title}\n\n"
    m1_content += f"*AIä¼˜åŒ–åçš„è§†é¢‘æ–‡ç¨¿ï¼Œå¯ç›´æ¥é˜…è¯»æ›¿ä»£è§†é¢‘*\n\n"
    m1_content += "---\n\n"
    m1_content += transcript.optimized_text
    m1_path.write_text(m1_content, encoding="utf-8")
    print(f"  M1 (è§†é¢‘æ–‡ç¨¿): {m1_path}")
